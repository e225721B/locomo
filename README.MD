# Data and Code for the **ACL 2024** Paper "**Evaluating Very Long-Term Conversational Memory of LLM Agents**"
**Authors**: [Adyasha Maharana](https://adymaharana.github.io/), [Dong-Ho Lee](https://www.danny-lee.info/), [Sergey Tulyakov](https://stulyakov.com/), [Mohit Bansal](https://www.cs.unc.edu/~mbansal/), [Francesco Barbieri](https://fvancesco.github.io/) and [Yuwei Fang](https://yuwfan.github.io/)

**Paper**: [arXiv](https://arxiv.org/abs/2402.17753)

## Data

We release LoCoMo, a high-quality evaluation benchmark consisting of *very* long-term conversational data. The benchmark consists of ten conversations. Each conversation is annotated for the **question-answering** and **event-summarization** tasks. Additionally, the dialogs in each conversation can be used for the **multimodal-dialog-generation** task. See statistics of the dataset in the Table below.

![image](./assets/d2_pruning_main.png)

The dataset can be found in the ```data/locomo10.zip``` file in this repository. Each file represents a single conversation and it's corresponding annotations; it contains the following fields: 
* `sample_id`: identifier for the sample
* `conversation`: 
    * This field contains a list of sessions (`session_<num>`) and their timestamps (`session_<num>_date_time`). The numbers `<num>` represent the chronological order of the sessions. * It also includes names of the two speakers i.e., `speaker_a` and `speaker_b`. 
    * A *turn* within each *session* contains the name of the `speaker`, the dialog id `dia_id`, and content of the dialog `text`. 
    * If the turn contains images, it also includes a link to the image `img_url`, caption generated by the [BLIP](https://huggingface.co/Salesforce/blip-image-captioning-large) model for the image `blip_caption` and the search query used by the third party module [icrawler](https://icrawler.readthedocs.io/en/latest/) to retrieve the image.
* `observations` (generated): This field contains observations for each of the sessions in `conversation` (`session_<num>_observation`). See below for the code to regenerate observations. These observations are used as one of the databases for evaluating retrieval-augmented generation i.e., RAG models in our paper.
* `session-summaries` (generated): This field contains session-level summaries for each session in `conversation` (`session_<num>_summary`). See below for the code to regenerate session-level summaries. These summaries are also used as one of the databases for evaluating RAG models in our paper.
* `event-summaries` (annotated): This field contains a list of significant events for each speaker within each session in `conversation` (`events_session_<num>`). These are the ground truth annotations for the event summarization task in the LoCoMo dataset.
* `qa` (annotated): This field contains the list of question-answer annotations for the question answering task in the LoCoMo dataset. Each sample contains `question`, `answer`, `category` label and a list of dialog ids that contain the answer i.e., `evidence`, when available.


**Note 1**: This release is a subset of the conversations released previously with our first Arxiv version in March 2024. The initial release contained 50 conversations. We sampled a subset of the data to retain the longest conversations with high-quality annotations and for cost-effective evaluation of closed-source LLMs.

**Note 2**: We do not release the images. However, the web URLs, captions and search queries for the images are included in the dataset.



## Setup

1. Create a virtual environment and activate it.
```
python3 -m venv env
source env/bin/activate
```
2. Install dependencies for all datasets except DataComp
```
python -m pip install -r requirements.txt -f https://download.pytorch.org/whl/cu113/torch_stable.html
```
For DataComp, see requirements of the [DataComp](https://github.com/mlfoundations/datacomp) codebase and additionally install [faiss](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md).

## Code

### Generate observations and summaries from conversations using `gpt-3.5-turbo`


### Evaluate open-source and closed-source LLMs on the Question Answering Task
* Configurations


* Evaluate OpenAI models 


### Acknowledgements

### Reference
Please cite our paper if you use LoCoMo in your works:
```bibtex

@article{maharana2024evaluating,
  title={Evaluating very long-term conversational memory of llm agents},
  author={Maharana, Adyasha and Lee, Dong-Ho and Tulyakov, Sergey and Bansal, Mohit and Barbieri, Francesco and Fang, Yuwei},
  journal={arXiv preprint arXiv:2402.17753},
  year={2024}
}
```